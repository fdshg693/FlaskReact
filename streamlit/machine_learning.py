from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import re
import time
import sys

import streamlit as st
import pandas as pd
from sklearn.datasets import load_iris, load_diabetes
from loguru import logger

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¨­å®šï¼ˆçµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®ãŸã‚ï¼‰
project_root = Path(__file__).resolve().parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))


class StreamlitMLApp:
    """Streamlitæ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""

    def __init__(self) -> None:
        """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–"""
        self.project_root = Path(__file__).resolve().parent.parent
        self.param_dir = self.project_root / "param"
        self.scaler_dir = self.project_root / "scaler"
        self.curve_log_dir = self.project_root / "curveLog"
        self.csv_log_dir = self.project_root / "csvLog"
        # äºˆæ¸¬ç”¨ï¼ˆIriså°‚ç”¨ï¼‰
        self.iris_feature_names = [
            "sepal_length",
            "sepal_width",
            "petal_length",
            "petal_width",
        ]

    # --------------------------------------------------
    # ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
    # --------------------------------------------------
    def get_param_models(self) -> List[Dict[str, str]]:
        """param/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã® .pth ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—

        Returns:
            List[Dict[str, str]]: ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        return self._collect_files(self.param_dir, "*.pth", name_mode="with_ext")

    def get_scalers(self) -> List[Dict[str, str]]:
        """
        æ—¢å­˜ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®ä¸€è¦§ã‚’å–å¾—

        Returns:
            List[Dict[str, str]]: ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """

        return self._collect_files(self.scaler_dir, "*.joblib", name_mode="stem")

    # --------------------------------------------------
    # å†…éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«å…±é€šåé›†ãƒ˜ãƒ«ãƒ‘ãƒ¼
    # --------------------------------------------------
    def _collect_files(
        self, directory: Path, pattern: str, name_mode: str = "stem"
    ) -> List[Dict[str, str]]:
        """æŒ‡å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’åé›†ï¼ˆé‡è¤‡ãƒ­ã‚¸ãƒƒã‚¯é›†ç´„ï¼‰

        Args:
            directory: èµ°æŸ»å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            pattern: glob ãƒ‘ã‚¿ãƒ¼ãƒ³ (ä¾‹: '*.pth')
            name_mode: 'stem' = æ‹¡å¼µå­é™¤å», 'with_ext' = æ‹¡å¼µå­ä»˜ã

        Returns:
            List[Dict[str, str]]: ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±è¾æ›¸ãƒªã‚¹ãƒˆ
        """
        items: List[Dict[str, str]] = []
        if not directory.exists():
            return items
        for file in directory.glob(pattern):
            if not file.is_file():
                continue
            try:
                name_value = file.stem if name_mode == "stem" else file.name
                items.append(
                    {
                        "name": name_value,  # UI ç”¨è¡¨ç¤ºå
                        "file": file.name,  # å¸¸ã«ãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«åæä¾›
                        "path": str(file),
                        "size": f"{file.stat().st_size / 1024:.2f} KB",
                        "modified": datetime.fromtimestamp(
                            file.stat().st_mtime
                        ).strftime("%Y-%m-%d %H:%M:%S"),
                    }
                )
            except Exception as e:  # noqa: BLE001
                logger.error(f"Failed to stat file {file}: {e}")
        # æ—¥ä»˜é™é †
        return sorted(items, key=lambda x: x["modified"], reverse=True)

    # --------------------------------------------------
    # å‰Šé™¤ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
    # --------------------------------------------------
    def _safe_delete(self, file_path: Path) -> bool:
        """æŒ‡å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®‰å…¨ã«å‰Šé™¤

        Args:
            file_path: å‰Šé™¤å¯¾è±¡

        Returns:
            bool: æˆåŠŸå¯å¦
        """
        try:
            # param/ ã‹ scaler/ é…ä¸‹ã®ã¿è¨±å¯
            if not (
                str(file_path).startswith(str(self.param_dir))
                or str(file_path).startswith(str(self.scaler_dir))
            ):
                logger.warning(f"Skip delete (outside allowed dir): {file_path}")
                return False
            if file_path.exists() and file_path.is_file():
                file_path.unlink()
                logger.info(f"Deleted file: {file_path}")
                return True
            logger.warning(f"File not found for delete: {file_path}")
            return False
        except Exception as e:  # noqa: BLE001
            logger.error(f"Delete failed {file_path}: {e}")
            return False

    def delete_models(self, names: List[str]) -> Dict[str, int]:
        """ãƒ¢ãƒ‡ãƒ«(.pth)ã‚’å‰Šé™¤

        Args:
            names: å‰Šé™¤å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«åãƒªã‚¹ãƒˆ

        Returns:
            Dict[str, int]: çµæœçµ±è¨ˆ
        """
        success = 0
        deleted_suffixes: List[str] = []
        for n in names:
            p = self.param_dir / n
            if self._safe_delete(p):
                success += 1
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ suffix æŠ½å‡º (models_{suffix}.pth)
                match = re.match(r"models_(.+)\.pth$", n)
                if match:
                    deleted_suffixes.append(match.group(1))
        if deleted_suffixes:
            self._remove_csv_rows_by_suffixes(deleted_suffixes)
        return {"requested": len(names), "deleted": success}

    def delete_scalers(self, names: List[str]) -> Dict[str, int]:
        """ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼(.joblib)ã‚’å‰Šé™¤

        Args:
            names: å‰Šé™¤å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«åãƒªã‚¹ãƒˆ

        Returns:
            Dict[str, int]: çµæœçµ±è¨ˆ
        """
        success = 0
        for n in names:
            p = self.scaler_dir / n
            if self._safe_delete(p):
                success += 1
        return {"requested": len(names), "deleted": success}

    def _remove_csv_rows_by_suffixes(self, suffixes: List[str]) -> None:
        """trained_model.csv ã‹ã‚‰æŒ‡å®š suffix ã®è¡Œã‚’å‰Šé™¤

        Args:
            suffixes: å‰Šé™¤å¯¾è±¡ file_suffix ã®ãƒªã‚¹ãƒˆ
        """
        trained_model_csv = self.project_root / "train_log" / "trained_model.csv"
        if not trained_model_csv.exists():
            return
        try:
            df = pd.read_csv(trained_model_csv)
            if "file_suffix" not in df.columns:
                logger.warning(
                    "trained_model.csv ã« file_suffix ã‚«ãƒ©ãƒ ãŒç„¡ã„ãŸã‚å‰Šé™¤åŒæœŸã‚’ã‚¹ã‚­ãƒƒãƒ—"
                )
                return
            before = len(df)
            df = df[~df["file_suffix"].isin(suffixes)]
            after = len(df)
            if after != before:
                df.to_csv(trained_model_csv, index=False)
                logger.info(
                    f"trained_model.csv ã‚’æ›´æ–°: {before - after} è¡Œå‰Šé™¤ (suffix: {suffixes})"
                )
        except Exception as e:  # noqa: BLE001
            logger.error(f"trained_model.csv æ›´æ–°å¤±æ•— (å‰Šé™¤åŒæœŸ): {e}")

    def validate_filename(self, filename: str) -> bool:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«åã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯

        Args:
            filename: ãƒã‚§ãƒƒã‚¯ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å

        Returns:
            bool: å¦¥å½“æ€§ã®çµæœ
        """
        if not filename:
            return False
        # ç¦æ­¢æ–‡å­—ã‚’ãƒã‚§ãƒƒã‚¯
        forbidden_chars = r'[<>:"/\\|?*]'
        if re.search(forbidden_chars, filename):
            return False
        # äºˆç´„èªã‚’ãƒã‚§ãƒƒã‚¯
        reserved_names = [
            "CON",
            "PRN",
            "AUX",
            "NUL",
            "COM1",
            "COM2",
            "COM3",
            "COM4",
            "COM5",
            "COM6",
            "COM7",
            "COM8",
            "COM9",
            "LPT1",
            "LPT2",
            "LPT3",
            "LPT4",
            "LPT5",
            "LPT6",
            "LPT7",
            "LPT8",
            "LPT9",
        ]
        if filename.upper() in reserved_names:
            return False
        return True

    def get_existing_models(self) -> List[Dict[str, str]]:
        """
        train_log/trained_model.csvã®å†…å®¹ã‚’å–å¾—ã—ã¦ãƒ¢ãƒ‡ãƒ«æƒ…å ±ãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã™

        Returns:
            List[Dict[str, str]]: ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        trained_model_csv = self.project_root / "train_log" / "trained_model.csv"
        models: List[Dict[str, str]] = []
        if trained_model_csv.exists():
            try:
                df = pd.read_csv(trained_model_csv)
                # å¿…è¦ãªã‚«ãƒ©ãƒ ã®ã¿æŠ½å‡ºï¼ˆname, accuracy, date, etc.ï¼‰
                for _, row in df.iterrows():
                    model_info = {key: str(row[key]) for key in df.columns}
                    models.append(model_info)
            except Exception as e:
                logger.error(f"Error reading trained_model.csv: {e}")
        return models

    def execute_training_with_custom_name(
        self, dataset: object, file_suffix: Optional[str] = None
    ) -> Dict[str, any]:
        """
        ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚¡ã‚¤ãƒ«åã§æ©Ÿæ¢°å­¦ç¿’ã‚’å®Ÿè¡Œ

        Args:
            dataset: å­¦ç¿’ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            file_suffix: ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆçœç•¥æ™‚ã¯ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼‰

        Returns:
            Dict[str, any]: å®Ÿè¡Œçµæœ
        """
        try:
            # å‹•çš„ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãŒè¨­å®šã•ã‚ŒãŸå¾Œï¼‰
            from machineLearning.ml_class import execute_machine_learning_pipeline

            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’åˆ¤æ–­
            dataset_name = "unknown"
            if hasattr(dataset, "filename") and "iris" in dataset.filename:
                dataset_name = "iris"
            elif hasattr(dataset, "filename") and "diabetes" in dataset.filename:
                dataset_name = "diabetes"
            elif hasattr(dataset, "DESCR") and "iris" in dataset.DESCR.lower():
                dataset_name = "iris"
            elif hasattr(dataset, "DESCR") and "diabetes" in dataset.DESCR.lower():
                dataset_name = "diabetes"

            # ã‚¨ãƒãƒƒã‚¯æ•° / å­¦ç¿’ç‡: ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®šãŒ session_state ã«ã‚ã‚Œã°åˆ©ç”¨
            epochs = 5
            learning_rate_override = None
            if "epochs_sidebar" in st.session_state:
                try:
                    epochs = int(st.session_state["epochs_sidebar"])
                except Exception:
                    pass
            if "lr_sidebar" in st.session_state:
                try:
                    lr_val = float(st.session_state["lr_sidebar"])
                    if lr_val > 0:
                        learning_rate_override = lr_val
                except Exception:
                    pass

            # ä¸€è²«ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’äº‹å‰æ±ºå®šï¼ˆscaler ã¨ model ã‚’æƒãˆã‚‹ï¼‰
            if file_suffix and self.validate_filename(file_suffix):
                effective_suffix = file_suffix
            else:
                effective_suffix = time.strftime("%Y%m%d_%H%M%S")

            # æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œï¼ˆscaler ã‚’ effective_suffix ã§ä¿å­˜ï¼‰
            model_wrapper, model, accuracy_history, loss_history = (
                execute_machine_learning_pipeline(
                    dataset,
                    epochs,
                    f"_{effective_suffix}",
                    learning_rate=learning_rate_override,
                )
            )

            # å­¦ç¿’æ›²ç·š + ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆæ—¢ã« suffix ã‚ã‚‹ã®ã§ãã®ã¾ã¾æ¸¡ã™ï¼‰
            from machineLearning.save_util import (
                save_model_and_learning_curves_with_custom_name,
            )

            save_model_and_learning_curves_with_custom_name(
                model,
                accuracy_history,
                loss_history,
                dataset_name,
                epochs,
                effective_suffix,
                self.project_root,
            )

            # è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ (åˆ†é¡: Accuracy, å›å¸°: R2)
            test_metric = model_wrapper.evaluate_model()
            metric_label = (
                "Accuracy"
                if getattr(model_wrapper, "is_classification", lambda: False)()
                else "R2"
            )

            return {
                "success": True,
                "accuracy": test_metric,  # å¾Œæ–¹äº’æ›ã‚­ãƒ¼åã‚’ç¶­æŒ
                "metric_label": metric_label,
                "timestamp": effective_suffix,
                "message": f"å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸã€‚{metric_label}: {test_metric:.3f}",
            }

        except Exception as e:
            logger.error(f"Training failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "message": f"å­¦ç¿’ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}",
            }

    def run_app(self) -> None:
        """Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
        st.set_page_config(
            page_title="æ©Ÿæ¢°å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¢ãƒ—ãƒª", page_icon="ğŸ¤–", layout="wide"
        )

        st.title("ğŸ¤– æ©Ÿæ¢°å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¢ãƒ—ãƒª")
        st.markdown("---")

        # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠ
        st.sidebar.header("è¨­å®š")
        dataset_choice = st.sidebar.selectbox(
            "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’é¸æŠ:", ["Iris (ã‚¢ã‚¤ãƒªã‚¹)", "Diabetes (ç³–å°¿ç—…)"]
        )

        st.session_state["epochs_sidebar"] = int(
            st.sidebar.number_input(
                "ã‚¨ãƒãƒƒã‚¯æ•°",
                min_value=1,
                max_value=200,
                value=5,
                step=1,
                help="å­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•°",
            )
        )
        st.session_state["lr_sidebar"] = float(
            st.sidebar.number_input(
                "å­¦ç¿’ç‡ (learning rate)",
                min_value=1e-5,
                max_value=1.0,
                value=0.1,
                step=0.01,
                format="%.5f",
                help="åˆ†é¡åˆæœŸå€¤0.1 / å›å¸°0.01 æ¨å¥¨ã€‚ã“ã“ã§ä¸Šæ›¸ãã€‚",
            )
        )
        st.sidebar.caption("å›å¸°(ç³–å°¿ç—…)ã§ã¯å­¦ç¿’ç‡ã‚’å°ã•ã‚ã«ã€‚")

        # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’2åˆ—ã«åˆ†å‰²
        col1, col2 = st.columns([1, 1])

        with col1:
            st.header("ğŸš€ æ–°ã—ã„å­¦ç¿’ã®å®Ÿè¡Œ")

            # ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚¡ã‚¤ãƒ«åå…¥åŠ›
            file_suffix = st.text_input(
                "ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆä»»æ„ï¼‰:",
                placeholder="ä¾‹: my_model_v1",
                help="æŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•çš„ã«ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒä½¿ç”¨ã•ã‚Œã¾ã™",
            )

            # ãƒ•ã‚¡ã‚¤ãƒ«åã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            if file_suffix:
                if self.validate_filename(file_suffix):
                    st.success("âœ… æœ‰åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«åã§ã™")
                else:
                    st.error(
                        "âŒ ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«åã§ã™ã€‚ç‰¹æ®Šæ–‡å­—ã‚„äºˆç´„èªã¯ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚"
                    )

            # å­¦ç¿’å®Ÿè¡Œãƒœã‚¿ãƒ³
            if st.button("ğŸ¯ æ©Ÿæ¢°å­¦ç¿’ã‚’å®Ÿè¡Œ", type="primary", use_container_width=True):
                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿
                if dataset_choice == "Iris (ã‚¢ã‚¤ãƒªã‚¹)":
                    dataset = load_iris()
                    st.info("ğŸŒ¸ Irisãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
                else:
                    dataset = load_diabetes()
                    st.info("ğŸ¥ Diabetesãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")

                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼è¡¨ç¤º
                progress_bar = st.progress(0)
                status_text = st.empty()

                # å­¦ç¿’å®Ÿè¡Œ
                with st.spinner("å­¦ç¿’ä¸­..."):
                    status_text.text("å­¦ç¿’ã‚’å®Ÿè¡Œä¸­...")
                    progress_bar.progress(50)

                    result = self.execute_training_with_custom_name(
                        dataset, file_suffix
                    )
                    progress_bar.progress(100)

                # çµæœè¡¨ç¤º
                if result["success"]:
                    st.success(result["message"])
                    st.balloons()

                    # è©³ç´°æƒ…å ±ã®è¡¨ç¤º
                    st.subheader("ğŸ“Š å­¦ç¿’çµæœ")
                    metric_col1, metric_col2 = st.columns(2)
                    with metric_col1:
                        label = result.get("metric_label", "ãƒ†ã‚¹ãƒˆç²¾åº¦")
                        st.metric(label, f"{result['accuracy']:.3f}")
                    with metric_col2:
                        st.metric("ä¿å­˜ID", result["timestamp"])

                    # è‡ªå‹•ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ï¼ˆæ—¢å­˜ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’æ›´æ–°ï¼‰
                    st.rerun()
                else:
                    st.error(result["message"])

        with col2:
            st.header("ğŸ“š æ—¢å­˜ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«")

            # ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã®å–å¾—ã¨è¡¨ç¤º
            existing_models = self.get_existing_models()
            existing_scalers = self.get_scalers()

            if existing_models:
                st.subheader("ğŸ·ï¸ ä¿å­˜æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« (train_log/trained_model.csv)")
                model_df = pd.DataFrame(existing_models)
                # ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ ã‚’è¡¨ç¤ºï¼ˆCSVã®å†…å®¹ã«ä¾å­˜ï¼‰
                st.dataframe(
                    model_df,
                    use_container_width=True,
                    hide_index=True,
                )

            else:
                st.info(
                    "ã¾ã å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å·¦å´ã§æ–°ã—ã„å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
                )

            if existing_scalers:
                st.subheader("ğŸ“ ä¿å­˜æ¸ˆã¿ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼")
                scaler_df = pd.DataFrame(existing_scalers)
                st.dataframe(
                    scaler_df[["name", "size", "modified"]],
                    use_container_width=True,
                    hide_index=True,
                )

            # --------------------------------------------------
            # å‰Šé™¤ UI
            # --------------------------------------------------
            st.subheader("ğŸ—‘ï¸ ãƒ¢ãƒ‡ãƒ« / ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼å‰Šé™¤")
            tab_m, tab_s = st.tabs(["ãƒ¢ãƒ‡ãƒ«å‰Šé™¤", "ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼å‰Šé™¤"])
            with tab_m:
                param_models = self.get_param_models()
                if not param_models:
                    st.info("param/ ã« .pth ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
                else:
                    selectable = [m["name"] for m in param_models]
                    sel_models = st.multiselect(
                        "å‰Šé™¤ã™ã‚‹ãƒ¢ãƒ‡ãƒ« (.pth) é¸æŠ",
                        selectable,
                        key="delete_models_select",
                    )
                    confirm_m = st.checkbox(
                        "ç¢ºèª: é¸æŠã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’å‰Šé™¤ã™ã‚‹", key="confirm_delete_models"
                    )
                    if st.button(
                        "é¸æŠãƒ¢ãƒ‡ãƒ«ã‚’å‰Šé™¤",
                        type="secondary",
                        disabled=not (sel_models and confirm_m),
                    ):
                        result = self.delete_models(sel_models)
                        st.success(
                            f"ãƒ¢ãƒ‡ãƒ«å‰Šé™¤å®Œäº†: {result['deleted']} / {result['requested']} ä»¶"
                        )
                        st.rerun()
            with tab_s:
                existing_scalers_list = self.get_scalers()
                if not existing_scalers_list:
                    st.info("scaler/ ã« .joblib ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
                else:
                    selectable_s = [s["file"] for s in existing_scalers_list]
                    sel_scalers = st.multiselect(
                        "å‰Šé™¤ã™ã‚‹ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ (.joblib) é¸æŠ",
                        selectable_s,
                        key="delete_scalers_select",
                    )
                    confirm_s = st.checkbox(
                        "ç¢ºèª: é¸æŠã—ãŸã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’å‰Šé™¤ã™ã‚‹",
                        key="confirm_delete_scalers",
                    )
                    if st.button(
                        "é¸æŠã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’å‰Šé™¤",
                        type="secondary",
                        disabled=not (sel_scalers and confirm_s),
                    ):
                        result = self.delete_scalers(sel_scalers)
                        st.success(
                            f"ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼å‰Šé™¤å®Œäº†: {result['deleted']} / {result['requested']} ä»¶"
                        )
                        st.rerun()

        # --- æ¨è«– (Iris) ã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
        st.markdown("---")
        with st.expander("ğŸ” Irisãƒ¢ãƒ‡ãƒ«ã§ãƒãƒƒãƒæ¨è«–ã‚’è¡Œã†", expanded=False):
            st.caption(
                "å­¦ç¿’æ¸ˆã¿Irisãƒ¢ãƒ‡ãƒ«(.pth)ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼(.joblib)ã‚’æŒ‡å®šã—ã¦è¤‡æ•°è¡Œã‚’ä¸€æ‹¬äºˆæ¸¬ã—ã¾ã™ã€‚"
            )

            # ãƒ¢ãƒ‡ãƒ« & ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ ãƒ•ã‚¡ã‚¤ãƒ«åˆ—æŒ™
            model_files = []
            if self.param_dir.exists():
                model_files = sorted(
                    [p for p in self.param_dir.glob("*.pth") if p.is_file()],
                    key=lambda p: p.stat().st_mtime,
                    reverse=True,
                )
            scaler_files = []
            if self.scaler_dir.exists():
                scaler_files = sorted(
                    [p for p in self.scaler_dir.glob("*.joblib") if p.is_file()],
                    key=lambda p: p.stat().st_mtime,
                    reverse=True,
                )

            if not model_files:
                st.info("param/ ã« .pth ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å­¦ç¿’å¾Œã«åˆ©ç”¨ã§ãã¾ã™ã€‚")
            if not scaler_files:
                st.info(
                    "scaler/ ã« .joblib ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å­¦ç¿’å¾Œã«åˆ©ç”¨ã§ãã¾ã™ã€‚"
                )

            if model_files and scaler_files:
                col_m1, col_m2 = st.columns(2)
                with col_m1:
                    selected_model = st.selectbox(
                        "ãƒ¢ãƒ‡ãƒ« (.pth)",
                        model_files,
                        format_func=lambda p: p.name,
                    )
                with col_m2:
                    selected_scaler = st.selectbox(
                        "ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ (.joblib)",
                        scaler_files,
                        format_func=lambda p: p.name,
                    )

                # ãƒ‡ãƒ¼ã‚¿å…¥åŠ›æ–¹æ³•
                st.markdown("#### å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ (ç‰¹å¾´é‡4åˆ—)")
                default_rows = 1
                n_rows = int(
                    st.number_input(
                        "è¡Œæ•°", min_value=1, max_value=50, value=default_rows, step=1
                    )
                )

                # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ– & è¡Œæ•°å¤‰æ›´æ™‚ã®ä¿æŒãƒ­ã‚¸ãƒƒã‚¯
                # NOTE: Streamlitã®ãƒãƒªã‚·ãƒ¼ä¸Šã€ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆkeyã«å¯¾ã™ã‚‹ st.session_state ç›´æ¥ä»£å…¥ã¯
                # StreamlitValueAssignmentNotAllowedError ã‚’å¼•ãèµ·ã“ã™ãŸã‚ã€
                # è¡¨ç¤ºç”¨(key)ã¨å†…éƒ¨ä¿æŒç”¨(key)ã‚’åˆ†é›¢ã™ã‚‹ã€‚
                widget_key = "iris_predict_df_editor"  # data_editor ç”¨
                storage_key = "iris_predict_df_data"  # å†…éƒ¨ä¿æŒç”¨ DataFrame

                if storage_key not in st.session_state:
                    # åˆæœŸåŒ–
                    st.session_state[storage_key] = pd.DataFrame(
                        [[0.0] * 4 for _ in range(n_rows)],
                        columns=self.iris_feature_names,
                    )
                else:
                    current_df = st.session_state[storage_key]
                    current_len = len(current_df)
                    # è¡Œæ•°å¢—åŠ : æ—¢å­˜å€¤ä¿æŒã—ã¤ã¤ã‚¼ãƒ­è¡Œè¿½åŠ 
                    if n_rows > current_len:
                        add_rows = pd.DataFrame(
                            [[0.0] * 4 for _ in range(n_rows - current_len)],
                            columns=self.iris_feature_names,
                        )
                        st.session_state[storage_key] = pd.concat(
                            [current_df, add_rows], ignore_index=True
                        )
                    # è¡Œæ•°æ¸›å°‘: å…ˆé ­ n_rows è¡Œã®ã¿ä¿æŒ
                    elif n_rows < current_len:
                        st.session_state[storage_key] = current_df.iloc[
                            :n_rows
                        ].reset_index(drop=True)

                edited_df = st.data_editor(
                    st.session_state[storage_key],
                    use_container_width=True,
                    num_rows="dynamic",
                    key=widget_key,
                    hide_index=True,
                )
                # ç·¨é›†çµæœã‚’å†…éƒ¨ä¿æŒDataFrameã«åæ˜ 
                # st.session_state[storage_key] = edited_df.copy()

                predict_btn = st.button(
                    "ğŸ§ª äºˆæ¸¬ã‚’å®Ÿè¡Œ", type="primary", use_container_width=False
                )

                if predict_btn:
                    # å…¥åŠ›æ¤œè¨¼
                    if edited_df.isnull().any().any():
                        st.error(
                            "æ¬ æå€¤ (NaN) ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚å…¨ã¦æ•°å€¤ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"
                        )
                    else:
                        # List[List[float]] ã«å¤‰æ›
                        try:
                            batch_data: List[List[float]] = (
                                edited_df[self.iris_feature_names]
                                .astype(float)
                                .values.tolist()
                            )
                        except Exception as e:  # å‹å¤‰æ›ã‚¨ãƒ©ãƒ¼
                            st.error(f"æ•°å€¤å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
                            batch_data = []

                        if batch_data:
                            with st.spinner("æ¨è«–ä¸­..."):
                                try:
                                    from machineLearning.eval_batch import (
                                        evaluate_iris_batch,
                                    )

                                    predictions = evaluate_iris_batch(
                                        batch_data,
                                        model_path=selected_model,
                                        scaler_path=selected_scaler,
                                    )
                                    if predictions:
                                        result_df = edited_df.copy()
                                        result_df["predicted_species"] = predictions
                                        st.success("æ¨è«–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                                        st.dataframe(
                                            result_df,
                                            use_container_width=True,
                                            hide_index=True,
                                        )
                                    else:
                                        st.error(
                                            "æ¨è«–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ¢ãƒ‡ãƒ«/ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼/å…¥åŠ›ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                                        )
                                except Exception as e:  # noqa: BLE001
                                    st.error(f"æ¨è«–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                                    logger.error(f"Inference error: {e}")

        # --- Diabetes å›å¸°æ¨è«–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
        st.markdown("---")
        with st.expander("ğŸ©º Diabetesãƒ¢ãƒ‡ãƒ«ã§ãƒãƒƒãƒæ¨è«– (å›å¸°)", expanded=False):
            st.caption(
                "å­¦ç¿’æ¸ˆã¿å›å¸°ãƒ¢ãƒ‡ãƒ«(.pth)ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼(.joblib)ã‚’é¸æŠã—ã€10ç‰¹å¾´é‡ã‚’å…¥åŠ›ã—ã¦äºˆæ¸¬ã€‚"
            )

            # ãƒ¢ãƒ‡ãƒ« & ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ ãƒ•ã‚¡ã‚¤ãƒ«åˆ—æŒ™
            model_files_reg = (
                sorted(
                    [p for p in self.param_dir.glob("*.pth")],
                    key=lambda p: p.stat().st_mtime,
                    reverse=True,
                )
                if self.param_dir.exists()
                else []
            )
            scaler_files_reg = (
                sorted(
                    [p for p in self.scaler_dir.glob("*.joblib")],
                    key=lambda p: p.stat().st_mtime,
                    reverse=True,
                )
                if self.scaler_dir.exists()
                else []
            )

            if not model_files_reg:
                st.info("param/ ã« .pth ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            if not scaler_files_reg:
                st.info("scaler/ ã« .joblib ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

            if model_files_reg and scaler_files_reg:
                rc1, rc2 = st.columns(2)
                with rc1:
                    selected_model_reg = st.selectbox(
                        "ãƒ¢ãƒ‡ãƒ« (.pth) (å›å¸°)",
                        model_files_reg,
                        format_func=lambda p: p.name,
                    )
                with rc2:
                    selected_scaler_reg = st.selectbox(
                        "ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ (.joblib) (å›å¸°)",
                        scaler_files_reg,
                        format_func=lambda p: p.name,
                    )

                # ãƒ‡ãƒ¼ã‚¿å…¥åŠ›æ–¹æ³•
                st.markdown("#### å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ (ç‰¹å¾´é‡10åˆ—)")
                default_rows_reg = 1
                n_rows_reg = int(
                    st.number_input(
                        "è¡Œæ•° (å›å¸°)",
                        min_value=1,
                        max_value=50,
                        value=default_rows_reg,
                        step=1,
                    )
                )

                # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ– & è¡Œæ•°å¤‰æ›´æ™‚ã®ä¿æŒãƒ­ã‚¸ãƒƒã‚¯
                widget_key_reg = "diabetes_predict_df_editor"  # data_editor ç”¨
                storage_key_reg = "diabetes_predict_df_data"  # å†…éƒ¨ä¿æŒç”¨ DataFrame

                if storage_key_reg not in st.session_state:
                    # åˆæœŸåŒ–
                    st.session_state[storage_key_reg] = pd.DataFrame(
                        [[0.0] * 10 for _ in range(n_rows_reg)],
                        columns=[f"f{i}" for i in range(10)],
                    )
                else:
                    cur_df = st.session_state[storage_key_reg]
                    cur_len = len(cur_df)
                    # è¡Œæ•°å¢—åŠ : æ—¢å­˜å€¤ä¿æŒã—ã¤ã¤ã‚¼ãƒ­è¡Œè¿½åŠ 
                    if n_rows_reg > cur_len:
                        add_rows = pd.DataFrame(
                            [[0.0] * 10 for _ in range(n_rows_reg - cur_len)],
                            columns=[f"f{i}" for i in range(10)],
                        )
                        st.session_state[storage_key_reg] = pd.concat(
                            [cur_df, add_rows], ignore_index=True
                        )
                    # è¡Œæ•°æ¸›å°‘: å…ˆé ­ n_rows è¡Œã®ã¿ä¿æŒ
                    elif n_rows_reg < cur_len:
                        st.session_state[storage_key_reg] = cur_df.iloc[
                            :n_rows_reg
                        ].reset_index(drop=True)

                edited_df_reg = st.data_editor(
                    st.session_state[storage_key_reg],
                    use_container_width=True,
                    num_rows="dynamic",
                    key=widget_key_reg,
                    hide_index=True,
                )

                if st.button(
                    "ğŸ§ª å›å¸°äºˆæ¸¬ã‚’å®Ÿè¡Œ", type="primary", use_container_width=False
                ):
                    # å…¥åŠ›æ¤œè¨¼
                    if edited_df_reg.isnull().any().any():
                        st.error(
                            "æ¬ æå€¤ (NaN) ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚å…¨ã¦æ•°å€¤ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"
                        )
                    else:
                        # List[List[float]] ã«å¤‰æ›
                        try:
                            reg_batch = edited_df_reg.astype(float).values.tolist()
                        except Exception as e:  # å‹å¤‰æ›ã‚¨ãƒ©ãƒ¼
                            st.error(f"æ•°å€¤å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
                            reg_batch = []

                        if reg_batch:
                            with st.spinner("æ¨è«–ä¸­..."):
                                try:
                                    from machineLearning.eval_batch import (
                                        evaluate_iris_batch,
                                    )

                                    preds = evaluate_iris_batch(
                                        reg_batch,
                                        model_path=selected_model_reg,
                                        scaler_path=selected_scaler_reg,
                                    )
                                    if preds:
                                        out_df = edited_df_reg.copy()
                                        out_df["predicted_value"] = preds
                                        st.success("å›å¸°æ¨è«–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                                        st.dataframe(
                                            out_df,
                                            use_container_width=True,
                                            hide_index=True,
                                        )
                                    else:
                                        st.error(
                                            "æ¨è«–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ¢ãƒ‡ãƒ«/ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼/å…¥åŠ›ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                                        )
                                except Exception as e:
                                    st.error(f"æ¨è«–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                                    logger.error(f"Regression inference error: {e}")

        # ãƒ•ãƒƒã‚¿ãƒ¼æƒ…å ±
        st.markdown("---")
        st.markdown(
            """
            <div style='text-align: center; color: gray;'>
                <p>ğŸ”§ powered by Streamlit | ğŸ§  æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ | ğŸ“ è‡ªå‹•ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜</p>
            </div>
            """,
            unsafe_allow_html=True,
        )


def main() -> None:
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    app = StreamlitMLApp()
    app.run_app()


if __name__ == "__main__":
    main()
