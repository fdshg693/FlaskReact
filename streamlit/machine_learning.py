from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import re
import sys

import streamlit as st
import pandas as pd
from sklearn.datasets import load_iris, load_diabetes
from loguru import logger

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¨­å®šï¼ˆçµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®ãŸã‚ï¼‰
project_root = Path(__file__).resolve().parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))


class StreamlitMLApp:
    """Streamlitæ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""

    def __init__(self) -> None:
        """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–"""
        self.project_root = Path(__file__).resolve().parent.parent
        self.param_dir = self.project_root / "param"
        self.scaler_dir = self.project_root / "scaler"
        self.curve_log_dir = self.project_root / "curveLog"
        self.csv_log_dir = self.project_root / "csvLog"

    def validate_filename(self, filename: str) -> bool:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«åã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯

        Args:
            filename: ãƒã‚§ãƒƒã‚¯ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å

        Returns:
            bool: å¦¥å½“æ€§ã®çµæœ
        """
        if not filename:
            return False
        # ç¦æ­¢æ–‡å­—ã‚’ãƒã‚§ãƒƒã‚¯
        forbidden_chars = r'[<>:"/\\|?*]'
        if re.search(forbidden_chars, filename):
            return False
        # äºˆç´„èªã‚’ãƒã‚§ãƒƒã‚¯
        reserved_names = [
            "CON",
            "PRN",
            "AUX",
            "NUL",
            "COM1",
            "COM2",
            "COM3",
            "COM4",
            "COM5",
            "COM6",
            "COM7",
            "COM8",
            "COM9",
            "LPT1",
            "LPT2",
            "LPT3",
            "LPT4",
            "LPT5",
            "LPT6",
            "LPT7",
            "LPT8",
            "LPT9",
        ]
        if filename.upper() in reserved_names:
            return False
        return True

    def get_existing_models(self) -> List[Dict[str, str]]:
        """
        æ—¢å­˜ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¸€è¦§ã‚’å–å¾—

        Returns:
            List[Dict[str, str]]: ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        models = []

        if self.param_dir.exists():
            for model_file in self.param_dir.glob("*.pth"):
                model_info = {
                    "name": model_file.stem,
                    "file": model_file.name,
                    "path": str(model_file),
                    "size": f"{model_file.stat().st_size / 1024:.2f} KB",
                    "modified": datetime.fromtimestamp(
                        model_file.stat().st_mtime
                    ).strftime("%Y-%m-%d %H:%M:%S"),
                }
                models.append(model_info)

        return sorted(models, key=lambda x: x["modified"], reverse=True)

    def get_existing_scalers(self) -> List[Dict[str, str]]:
        """
        æ—¢å­˜ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®ä¸€è¦§ã‚’å–å¾—

        Returns:
            List[Dict[str, str]]: ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        scalers = []

        if self.scaler_dir.exists():
            for scaler_file in self.scaler_dir.glob("*.joblib"):
                scaler_info = {
                    "name": scaler_file.stem,
                    "file": scaler_file.name,
                    "path": str(scaler_file),
                    "size": f"{scaler_file.stat().st_size / 1024:.2f} KB",
                    "modified": datetime.fromtimestamp(
                        scaler_file.stat().st_mtime
                    ).strftime("%Y-%m-%d %H:%M:%S"),
                }
                scalers.append(scaler_info)

        return sorted(scalers, key=lambda x: x["modified"], reverse=True)

    def execute_training_with_custom_name(
        self, dataset: object, custom_name: Optional[str] = None
    ) -> Dict[str, any]:
        """
        ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚¡ã‚¤ãƒ«åã§æ©Ÿæ¢°å­¦ç¿’ã‚’å®Ÿè¡Œ

        Args:
            dataset: å­¦ç¿’ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            custom_name: ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆçœç•¥æ™‚ã¯ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼‰

        Returns:
            Dict[str, any]: å®Ÿè¡Œçµæœ
        """
        try:
            # å‹•çš„ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãŒè¨­å®šã•ã‚ŒãŸå¾Œï¼‰
            from machineLearning.ml_class import execute_machine_learning_pipeline

            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’åˆ¤æ–­
            dataset_name = "unknown"
            if hasattr(dataset, "filename") and "iris" in dataset.filename:
                dataset_name = "iris"
            elif hasattr(dataset, "filename") and "diabetes" in dataset.filename:
                dataset_name = "diabetes"
            elif hasattr(dataset, "DESCR") and "iris" in dataset.DESCR.lower():
                dataset_name = "iris"
            elif hasattr(dataset, "DESCR") and "diabetes" in dataset.DESCR.lower():
                dataset_name = "diabetes"

            # ã‚¨ãƒãƒƒã‚¯æ•°ã‚’è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰
            epochs = 5

            # æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ
            classifier, model, accuracy_history, loss_history = (
                execute_machine_learning_pipeline(dataset, epochs)
            )

            # ã‚«ã‚¹ã‚¿ãƒ åã®å‡¦ç†ã¨å­¦ç¿’çµæœã®ä¿å­˜
            from machineLearning.save_util import (
                save_model_and_learning_curves_with_custom_name,
            )

            if custom_name and self.validate_filename(custom_name):
                file_suffix = save_model_and_learning_curves_with_custom_name(
                    model,
                    accuracy_history,
                    loss_history,
                    dataset_name,
                    epochs,
                    custom_name,
                    self.project_root,
                )
            else:
                file_suffix = save_model_and_learning_curves_with_custom_name(
                    model,
                    accuracy_history,
                    loss_history,
                    dataset_name,
                    epochs,
                    None,
                    self.project_root,
                )

            # ãƒ†ã‚¹ãƒˆç²¾åº¦ã®è©•ä¾¡
            test_accuracy = classifier.evaluate_model()

            return {
                "success": True,
                "accuracy": test_accuracy,
                "timestamp": file_suffix,
                "message": f"å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_accuracy:.3f}",
            }

        except Exception as e:
            logger.error(f"Training failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "message": f"å­¦ç¿’ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}",
            }

    def run_app(self) -> None:
        """Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
        st.set_page_config(
            page_title="æ©Ÿæ¢°å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¢ãƒ—ãƒª", page_icon="ğŸ¤–", layout="wide"
        )

        st.title("ğŸ¤– æ©Ÿæ¢°å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¢ãƒ—ãƒª")
        st.markdown("---")

        # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠ
        st.sidebar.header("è¨­å®š")
        dataset_choice = st.sidebar.selectbox(
            "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’é¸æŠ:", ["Iris (ã‚¢ã‚¤ãƒªã‚¹)", "Diabetes (ç³–å°¿ç—…)"]
        )

        # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’2åˆ—ã«åˆ†å‰²
        col1, col2 = st.columns([1, 1])

        with col1:
            st.header("ğŸš€ æ–°ã—ã„å­¦ç¿’ã®å®Ÿè¡Œ")

            # ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚¡ã‚¤ãƒ«åå…¥åŠ›
            custom_name = st.text_input(
                "ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆä»»æ„ï¼‰:",
                placeholder="ä¾‹: my_model_v1",
                help="æŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•çš„ã«ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒä½¿ç”¨ã•ã‚Œã¾ã™",
            )

            # ãƒ•ã‚¡ã‚¤ãƒ«åã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            if custom_name:
                if self.validate_filename(custom_name):
                    st.success("âœ… æœ‰åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«åã§ã™")
                else:
                    st.error(
                        "âŒ ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«åã§ã™ã€‚ç‰¹æ®Šæ–‡å­—ã‚„äºˆç´„èªã¯ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚"
                    )

            # å­¦ç¿’å®Ÿè¡Œãƒœã‚¿ãƒ³
            if st.button("ğŸ¯ æ©Ÿæ¢°å­¦ç¿’ã‚’å®Ÿè¡Œ", type="primary", use_container_width=True):
                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿
                if dataset_choice == "Iris (ã‚¢ã‚¤ãƒªã‚¹)":
                    dataset = load_iris()
                    st.info("ğŸŒ¸ Irisãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
                else:
                    dataset = load_diabetes()
                    st.info("ğŸ¥ Diabetesãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")

                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼è¡¨ç¤º
                progress_bar = st.progress(0)
                status_text = st.empty()

                # å­¦ç¿’å®Ÿè¡Œ
                with st.spinner("å­¦ç¿’ä¸­..."):
                    status_text.text("å­¦ç¿’ã‚’å®Ÿè¡Œä¸­...")
                    progress_bar.progress(50)

                    result = self.execute_training_with_custom_name(
                        dataset, custom_name
                    )
                    progress_bar.progress(100)

                # çµæœè¡¨ç¤º
                if result["success"]:
                    st.success(result["message"])
                    st.balloons()

                    # è©³ç´°æƒ…å ±ã®è¡¨ç¤º
                    st.subheader("ğŸ“Š å­¦ç¿’çµæœ")
                    metric_col1, metric_col2 = st.columns(2)
                    with metric_col1:
                        st.metric("ãƒ†ã‚¹ãƒˆç²¾åº¦", f"{result['accuracy']:.3f}")
                    with metric_col2:
                        st.metric("ä¿å­˜ID", result["timestamp"])

                    # è‡ªå‹•ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ï¼ˆæ—¢å­˜ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’æ›´æ–°ï¼‰
                    st.rerun()
                else:
                    st.error(result["message"])

        with col2:
            st.header("ğŸ“š æ—¢å­˜ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«")

            # ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã®å–å¾—ã¨è¡¨ç¤º
            existing_models = self.get_existing_models()
            existing_scalers = self.get_existing_scalers()

            if existing_models:
                st.subheader("ğŸ·ï¸ ä¿å­˜æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«")
                model_df = pd.DataFrame(existing_models)
                st.dataframe(
                    model_df[["name", "size", "modified"]],
                    use_container_width=True,
                    hide_index=True,
                )

                # æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°è¡¨ç¤º
                latest_model = existing_models[0]
                with st.expander(f"ğŸ“„ æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°: {latest_model['name']}"):
                    st.write(f"**ãƒ•ã‚¡ã‚¤ãƒ«å:** {latest_model['file']}")
                    st.write(f"**ã‚µã‚¤ã‚º:** {latest_model['size']}")
                    st.write(f"**æ›´æ–°æ—¥æ™‚:** {latest_model['modified']}")
                    st.write(f"**ãƒ‘ã‚¹:** {latest_model['path']}")
            else:
                st.info(
                    "ã¾ã å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å·¦å´ã§æ–°ã—ã„å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
                )

            if existing_scalers:
                st.subheader("ğŸ“ ä¿å­˜æ¸ˆã¿ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼")
                scaler_df = pd.DataFrame(existing_scalers)
                st.dataframe(
                    scaler_df[["name", "size", "modified"]],
                    use_container_width=True,
                    hide_index=True,
                )

        # ãƒ•ãƒƒã‚¿ãƒ¼æƒ…å ±
        st.markdown("---")
        st.markdown(
            """
            <div style='text-align: center; color: gray;'>
                <p>ğŸ”§ powered by Streamlit | ğŸ§  æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ | ğŸ“ è‡ªå‹•ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜</p>
            </div>
            """,
            unsafe_allow_html=True,
        )


def main() -> None:
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    app = StreamlitMLApp()
    app.run_app()


if __name__ == "__main__":
    main()
