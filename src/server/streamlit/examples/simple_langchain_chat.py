"""
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®ã‚·ãƒ³ã‚°ãƒ«ã‚¿ãƒ¼ãƒ³Q&Aãƒãƒ£ãƒƒãƒˆã‚’æä¾›ã™ã‚‹Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
"""

from __future__ import annotations

import streamlit as st

from config import load_dotenv_workspace


def main() -> None:
    # .envã‚’èª­ã¿è¾¼ã‚€ï¼ˆæ—¢å­˜ã®ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒå¤‰æ•°ã¯ä¸Šæ›¸ãã—ãªã„ï¼‰
    load_dotenv_workspace()

    # å®Ÿè¡Œæ™‚ã« import ã—ã¦ã€import-time å‰¯ä½œç”¨ã‚’é¿ã‘ã‚‹
    from llm.langchain_custom.examples.simple_call import main as call_simple_model

    st.set_page_config(page_title="LangChain Chat", page_icon="ğŸ’¬", layout="centered")

    st.title("LangChain Chat")
    st.caption(
        "Single-turn Q&A Streamlit chat using your shared LangChain model initializer."
    )

    with st.sidebar:
        with st.expander("ğŸ” Debug: Session State", expanded=False):
            st.json(dict(st.session_state))

    prompt = st.chat_input("Type a message")

    # On submit, render only the current exchange.
    if prompt:
        st.chat_message("user").markdown(prompt)

        with st.chat_message("assistant"):
            body = st.empty()  # å›ç­”è¡¨ç¤ºå°‚ç”¨ã®å ´æ‰€ã‚’ç¢ºä¿
            body.markdown("")  # å…ˆã«ç©ºã§ä¸Šæ›¸ãï¼ˆã“ã‚Œã§å‰å›ã®æ®‹åƒã‚’æ¶ˆã™ï¼‰
            with st.spinner("Thinking..."):
                try:
                    answer = call_simple_model(prompt)
                except Exception as exc:  # noqa: BLE001
                    st.error(f"Model call failed: {exc}")
                    st.stop()

            body.markdown(answer)

        st.session_state.last_prompt = prompt
        st.session_state.last_answer = answer
        st.stop()

    # Otherwise, show only the last exchange.
    last_prompt = st.session_state.get("last_prompt")
    last_answer = st.session_state.get("last_answer")

    if last_prompt:
        with st.chat_message("user"):
            st.markdown(last_prompt)
    if last_answer:
        with st.chat_message("assistant"):
            st.markdown(last_answer)


if __name__ == "__main__":
    main()
